{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "asr_lab_4_new.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5ce18e80aeec4c20abdebbc1d0b25983": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_eaf60dd5e45d4c748523d6a60f8cd770",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_def6aa5277d647ab8fa387d247a65a3b",
              "IPY_MODEL_7b4c809edc334c51b8074b6554bb6671"
            ]
          }
        },
        "eaf60dd5e45d4c748523d6a60f8cd770": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "def6aa5277d647ab8fa387d247a65a3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_36d1feef3fb24db7b0286dfd78844eca",
            "_dom_classes": [],
            "description": "  1%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 6387309499,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 56688640,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0e8054e683174dfeb6b4fa6718c26c69"
          }
        },
        "7b4c809edc334c51b8074b6554bb6671": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d4b8fa1135224ff69aecc03f44cd1bd1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 54.1M/5.95G [17:56&lt;33:23:55, 52.7kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_11b73187895142fda36866d4e940d030"
          }
        },
        "36d1feef3fb24db7b0286dfd78844eca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0e8054e683174dfeb6b4fa6718c26c69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d4b8fa1135224ff69aecc03f44cd1bd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "11b73187895142fda36866d4e940d030": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SavkinEgor/MADE_ML_Speech/blob/main/SPEECH_asr_lab_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RcyxmRJGqlY"
      },
      "source": [
        "# Практика №4\n",
        "\n",
        "Теперь мы построим и обучим простую end-to-end модель. Будем работать с пропатченной версией уже готового [пайплайна](https://www.assemblyai.com/blog/end-to-end-speech-recognition-pytorch). Также нам пригодится [ESPnet](https://github.com/espnet/espnet) для использования модели [Transformer](http://jalammar.github.io/illustrated-transformer/) в качестве энкодера."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDbO_rrWGq7j"
      },
      "source": [
        "### Bootstrap"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "st9pPbfJXAz2",
        "outputId": "069bca23-497a-4164-aeb8-7cc82bb41d7d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.activity.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fexperimentsandconfigs%20https%3a%2f%2fwww.googleapis.com%2fauth%2fphotos.native&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "4/1AY0e-g7xtHSKtXm-uaCqNjm0lmR-5-NglGmPys_OG96kpGd9IE1Xxghtoew\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzJyomV1JaLp"
      },
      "source": [
        "!pip install torchaudio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TROAsHTXHWik"
      },
      "source": [
        "!gdown --id '1skrVbNyrhBLeceGS9CV9uIw_gvo1JiA6'\n",
        "\n",
        "!unzip -q lab4.zip\n",
        "!rm -rf lab4.zip sample_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iU_UANDG2Fz-",
        "outputId": "bbc9b74e-847c-49a9-f7e0-a54447186231"
      },
      "source": [
        "%cd lab4"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/lab4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4wcCtkIH2dn"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "from utils import TextTransform\n",
        "from utils import cer\n",
        "from utils import wer\n",
        "\n",
        "from espnet.nets.pytorch_backend.transformer.embedding import PositionalEncoding\n",
        "from espnet.nets.pytorch_backend.transformer.encoder_layer import EncoderLayer\n",
        "from espnet.nets.pytorch_backend.transformer.repeat import repeat\n",
        "from espnet.nets.pytorch_backend.transformer.attention import MultiHeadedAttention\n",
        "from espnet.nets.pytorch_backend.transformer.positionwise_feed_forward import PositionwiseFeedForward\n",
        "from espnet.nets.pytorch_backend.transformer.layer_norm import LayerNorm\n",
        "from espnet.nets.pytorch_backend.nets_utils import make_pad_mask"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-lET8WtW9G-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "934e8173-47e5-49c0-ed30-1a69e6a2bf30"
      },
      "source": [
        "torch.cuda.get_device_name()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tesla K80'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaESUZiHJgfN"
      },
      "source": [
        "train_audio_transforms = torch.nn.Sequential(\n",
        "    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_fft=400, hop_length=160, n_mels=80),\n",
        "    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
        "    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
        ")\n",
        "\n",
        "valid_audio_transforms = torchaudio.transforms.MelSpectrogram(sample_rate=16000,\n",
        "                                                              n_fft=400,\n",
        "                                                              hop_length=160,\n",
        "                                                              n_mels=80)\n",
        "\n",
        "# text_transform = TextTransform()\n",
        "# text_transform = TextTransformBPE()\n",
        "\n",
        "# #-----------------------------TODO №2-----------------------------------\n",
        "# # Заменить графемный токенайзер на сабвордовый TextTransformBPE\n",
        "# #-----------------------------------------------------------------------\n",
        "\n",
        "\n",
        "def data_processing(data, data_type=\"train\"):\n",
        "    spectrograms = []\n",
        "    labels = []\n",
        "    input_lengths = []\n",
        "    label_lengths = []\n",
        "    for (waveform, _, utterance, _, _, _) in data:\n",
        "        if data_type == 'train':\n",
        "            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        elif data_type == 'valid':\n",
        "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        else:\n",
        "            raise Exception('data_type should be train or valid')\n",
        "        spectrograms.append(spec)\n",
        "        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n",
        "        labels.append(label)\n",
        "        input_lengths.append(spec.shape[0])\n",
        "        label_lengths.append(len(label))\n",
        "\n",
        "    spectrograms = torch.nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
        "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
        "\n",
        "    return spectrograms, labels, input_lengths, label_lengths\n",
        "\n",
        "\n",
        "def GreedyDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n",
        "    # можно бимсерч прикрутить\n",
        "    arg_maxes = torch.argmax(output, dim=2)\n",
        "    decodes = []\n",
        "    targets = []\n",
        "    for i, args in enumerate(arg_maxes):\n",
        "        decode = []\n",
        "        targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
        "        for j, index in enumerate(args):\n",
        "            if index != blank_label:\n",
        "                if collapse_repeated and j != 0 and index == args[j -1]:\n",
        "                    continue\n",
        "                decode.append(index.item())\n",
        "        decodes.append(text_transform.int_to_text(decode))\n",
        "    return decodes, targets"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OqoVLnrJsCV"
      },
      "source": [
        "class TransformerModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=80,\n",
        "        output_size=29,\n",
        "        conv2d_filters=32,\n",
        "        attention_dim=360,\n",
        "        attention_heads=8,\n",
        "        feedforward_dim=1024,\n",
        "        num_layers=10,\n",
        "        dropout=0.1,\n",
        "    ):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        \n",
        "        self.conv_in = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(1, conv2d_filters, kernel_size=(3,3), stride=(2,2), padding=(1,1)),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Conv2d(conv2d_filters, conv2d_filters, kernel_size=(3,3), stride=(2,2), padding=(1,1)),\n",
        "            torch.nn.ReLU(),\n",
        "        )\n",
        "        self.conv_out = torch.nn.Sequential(\n",
        "            torch.nn.Linear(conv2d_filters * ((input_size // 2) // 2), attention_dim),\n",
        "            PositionalEncoding(attention_dim, 0.1),\n",
        "        )\n",
        "        positionwise_layer = PositionwiseFeedForward\n",
        "        positionwise_layer_args = (attention_dim, feedforward_dim, dropout)\n",
        "        self.encoder_layer = repeat(\n",
        "            num_layers,\n",
        "            lambda lnum: EncoderLayer(\n",
        "                attention_dim,\n",
        "                MultiHeadedAttention(\n",
        "                    attention_heads, attention_dim, dropout\n",
        "                ),\n",
        "                positionwise_layer(*positionwise_layer_args),\n",
        "                dropout,\n",
        "                normalize_before=True,\n",
        "                concat_after=False,\n",
        "            ),\n",
        "        )\n",
        "        self.after_norm = LayerNorm(attention_dim)\n",
        "        self.final_layer = torch.nn.Linear(attention_dim, output_size)\n",
        "\n",
        "    def forward(self, x, ilens):\n",
        "        x = x.unsqueeze(1)  # (b, c, t, f)\n",
        "        x = self.conv_in(x)\n",
        "        b, c, t, f = x.size()\n",
        "        x = self.conv_out(x.transpose(1, 2).contiguous().view(b, t, c * f))\n",
        "        masks = (~make_pad_mask(ilens)[:, None, :])[:, :, ::4].to(x.device)\n",
        "        x, _ = self.encoder_layer(x, masks)\n",
        "        x = self.after_norm(x)\n",
        "        x = self.final_layer(x)\n",
        "        return x"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2p_8IjeKkqq"
      },
      "source": [
        "def train(model, device, train_loader, criterion, optimizer, scheduler, epoch):\n",
        "    model.train()\n",
        "    data_len = len(train_loader.dataset)\n",
        "\n",
        "    for batch_idx, _data in enumerate(train_loader):\n",
        "        spectrograms, labels, input_lengths, label_lengths = _data \n",
        "        spectrograms, labels = spectrograms[:, :, :,:max(input_lengths)].to(device), labels.to(device) #(batch, 1, feat_dim, time)\n",
        "        spectrograms = spectrograms.squeeze(1).transpose(1,2) # (batch, time, feat_dim,)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(spectrograms, input_lengths)  # (batch, time, n_classes)\n",
        "        output = F.log_softmax(output, dim=2)\n",
        "        output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "        input_lengths = [x // 4 for x in input_lengths]\n",
        "\n",
        "        loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        if batch_idx % 100 == 0 or batch_idx == data_len:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tLR: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(spectrograms), data_len,\n",
        "                100. * batch_idx / len(train_loader), loss.item(), scheduler.get_last_lr()[0]))\n",
        "\n",
        "\n",
        "def test(model, device, test_loader, criterion, epoch):\n",
        "    print('\\nevaluating...')\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_cer, test_wer = [], []\n",
        "    with torch.no_grad():\n",
        "        for i, _data in enumerate(test_loader):\n",
        "            spectrograms, labels, input_lengths, label_lengths = _data \n",
        "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "            spectrograms = spectrograms.squeeze(1).transpose(1,2) # (batch time, feat_dim,)\n",
        "            \n",
        "            output = model(spectrograms, input_lengths)  # (batch, time, n_class)\n",
        "            output = F.log_softmax(output, dim=2)\n",
        "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "            input_lengths = [x // 4 for x in input_lengths]\n",
        "\n",
        "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "            test_loss += loss.item() / len(test_loader)\n",
        "\n",
        "            decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
        "            for j in range(len(decoded_preds)):\n",
        "                test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
        "                test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
        "\n",
        "    avg_cer = sum(test_cer)/len(test_cer)\n",
        "    avg_wer = sum(test_wer)/len(test_wer)\n",
        "\n",
        "    print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzEbtsB1LKsh"
      },
      "source": [
        "def main(learning_rate=1e-5, batch_size=20, test_batch_size=7, epochs=10,\n",
        "        train_url=\"train-clean-100\", test_url=\"test-clean\", vocab_size=29, transformer=\"transformer\"):\n",
        "    \n",
        "    hparams = {\n",
        "        \"input_size\": 80,\n",
        "        \"output_size\": vocab_size,\n",
        "        \"conv2d_filters\": 32,\n",
        "        \"attention_dim\": 360,\n",
        "        \"attention_heads\": 8,\n",
        "        \"cnn_module_kernel\": 31,\n",
        "        \"feedforward_dim\": 1024,\n",
        "        \"num_layers\":10,\n",
        "        \"dropout\": 0.1,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"epochs\": epochs\n",
        "    }\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    #use_cuda = False\n",
        "    torch.manual_seed(7)\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    if not os.path.isdir(\"./data\"):\n",
        "        os.makedirs(\"./data\")\n",
        "\n",
        "    train_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=train_url, download=True)\n",
        "    test_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=test_url, download=True)\n",
        "\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "    train_loader = data.DataLoader(dataset=train_dataset,\n",
        "                                batch_size=hparams['batch_size'],\n",
        "                                shuffle=True,\n",
        "                                collate_fn=lambda x: data_processing(x, 'train'),\n",
        "                                **kwargs)\n",
        "    test_loader = data.DataLoader(dataset=test_dataset,\n",
        "                                batch_size=test_batch_size,\n",
        "                                shuffle=False,\n",
        "                                collate_fn=lambda x: data_processing(x, 'valid'),\n",
        "                                **kwargs)\n",
        "    \n",
        "    if transformer == \"transformer\":\n",
        "        model = TransformerModel(\n",
        "            hparams['input_size'],\n",
        "            hparams['output_size'],\n",
        "            hparams['conv2d_filters'],\n",
        "            hparams['attention_dim'],\n",
        "            hparams['attention_heads'],\n",
        "            hparams['feedforward_dim'],\n",
        "            hparams['num_layers'],\n",
        "            hparams['dropout']).to(device)\n",
        "\n",
        "    elif transformer == \"conformer\":\n",
        "        model = ConformerModel(\n",
        "            hparams['input_size'],\n",
        "            hparams['output_size'],\n",
        "            hparams['conv2d_filters'],\n",
        "            hparams['attention_dim'],\n",
        "            hparams['attention_heads'],\n",
        "            hparams['cnn_module_kernel'],\n",
        "            hparams['feedforward_dim'],\n",
        "            hparams['num_layers'],\n",
        "            hparams['dropout']).to(device)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "\n",
        "    print(model)\n",
        "    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
        "    criterion = torch.nn.CTCLoss(blank=vocab_size-1, zero_infinity=False).to(device)\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n",
        "                                            steps_per_epoch=int(len(train_loader)),\n",
        "                                            epochs=hparams['epochs'],\n",
        "                                            anneal_strategy='linear')\n",
        "    \n",
        "    for epoch in range(1, epochs + 1):\n",
        "        !date\n",
        "        train(model, device, train_loader, criterion, optimizer, scheduler, epoch)\n",
        "        test(model, device, test_loader, criterion, epoch)\n",
        "        torch.save(model, \"/content/drive/MyDrive/MADE_ex/lab4/s2t_model.pth\")\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eExZLsUiLeXk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389,
          "referenced_widgets": [
            "5ce18e80aeec4c20abdebbc1d0b25983",
            "eaf60dd5e45d4c748523d6a60f8cd770",
            "def6aa5277d647ab8fa387d247a65a3b",
            "7b4c809edc334c51b8074b6554bb6671",
            "36d1feef3fb24db7b0286dfd78844eca",
            "0e8054e683174dfeb6b4fa6718c26c69",
            "d4b8fa1135224ff69aecc03f44cd1bd1",
            "11b73187895142fda36866d4e940d030"
          ]
        },
        "outputId": "de32121c-e5a7-4493-c8b0-569147f02321"
      },
      "source": [
        "learning_rate = 1e-3\n",
        "# batch_size = 7\n",
        "batch_size = 5\n",
        "# test_batch_size = 7\n",
        "test_batch_size = 5\n",
        "epochs = 10\n",
        "libri_train_set = \"train-clean-100\"\n",
        "libri_test_set = \"test-clean\"\n",
        "\n",
        "main(learning_rate, batch_size, test_batch_size, epochs, libri_train_set, libri_test_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5ce18e80aeec4c20abdebbc1d0b25983",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=6387309499.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-f40dc09914fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mlibri_test_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"test-clean\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibri_train_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibri_test_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-9a8530434ee4>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(learning_rate, batch_size, test_batch_size, epochs, train_url, test_url, vocab_size, transformer)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLIBRISPEECH\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLIBRISPEECH\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchaudio/datasets/librispeech.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, url, folder_in_archive, download)\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                     \u001b[0mchecksum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_CHECKSUMS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                     \u001b[0mdownload_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhash_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchecksum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m                 \u001b[0mextract_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchaudio/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_url\u001b[0;34m(url, download_folder, filename, hash_value, hash_type, progress_bar, resume)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfpointer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstream_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_byte\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m             \u001b[0mfpointer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchaudio/datasets/utils.py\u001b[0m in \u001b[0;36mstream_url\u001b[0;34m(url, start_byte, block_size, progress_bar)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mnum_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupointer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 461\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    462\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYtzzigcX29F"
      },
      "source": [
        "torch.save(model, \"/content/drive/MyDrive/MADE_ex/lab4/default_model.pth\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZH5kQn8Abyj"
      },
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 12\n",
        "test_batch_size = 12\n",
        "epochs = 8\n",
        "libri_train_set = \"train-clean-100\"\n",
        "libri_test_set = \"test-clean\"\n",
        "\n",
        "vocab_size=4001\n",
        "\n",
        "train_url=\"train-clean-100\"\n",
        "test_url=\"test-clean\"\n",
        "\n",
        "transformer = \"transformer\""
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehkLNBZ6ACWT",
        "outputId": "946f9515-ad6a-4cb5-95bf-76247031eaf9"
      },
      "source": [
        "\n",
        "hparams = {\n",
        "    \"input_size\": 80,\n",
        "    \"output_size\": vocab_size,\n",
        "    \"conv2d_filters\": 32,\n",
        "    \"attention_dim\": 360,\n",
        "    \"attention_heads\": 8,\n",
        "    \"cnn_module_kernel\": 31,\n",
        "    \"feedforward_dim\": 1024,\n",
        "    \"num_layers\":10,\n",
        "    \"dropout\": 0.1,\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"batch_size\": batch_size,\n",
        "    \"epochs\": epochs\n",
        "}\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "#use_cuda = False\n",
        "torch.manual_seed(7)\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "if not os.path.isdir(\"./data\"):\n",
        "    os.makedirs(\"./data\")\n",
        "\n",
        "train_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=train_url, download=True)\n",
        "test_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=test_url, download=True)\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "train_loader = data.DataLoader(dataset=train_dataset,\n",
        "                            batch_size=hparams['batch_size'],\n",
        "                            shuffle=True,\n",
        "                            collate_fn=lambda x: data_processing(x, 'train'),\n",
        "                            **kwargs)\n",
        "test_loader = data.DataLoader(dataset=test_dataset,\n",
        "                            batch_size=test_batch_size,\n",
        "                            shuffle=False,\n",
        "                            collate_fn=lambda x: data_processing(x, 'valid'),\n",
        "                            **kwargs)\n",
        "\n",
        "if transformer == \"transformer\":\n",
        "    model = TransformerModel(\n",
        "        hparams['input_size'],\n",
        "        hparams['output_size'],\n",
        "        hparams['conv2d_filters'],\n",
        "        hparams['attention_dim'],\n",
        "        hparams['attention_heads'],\n",
        "        hparams['feedforward_dim'],\n",
        "        hparams['num_layers'],\n",
        "        hparams['dropout']).to(device)\n",
        "\n",
        "elif transformer == \"conformer\":\n",
        "    model = ConformerModel(\n",
        "        hparams['input_size'],\n",
        "        hparams['output_size'],\n",
        "        hparams['conv2d_filters'],\n",
        "        hparams['attention_dim'],\n",
        "        hparams['attention_heads'],\n",
        "        hparams['cnn_module_kernel'],\n",
        "        hparams['feedforward_dim'],\n",
        "        hparams['num_layers'],\n",
        "        hparams['dropout']).to(device)\n",
        "else:\n",
        "    raise NotImplementedError\n",
        "\n",
        "print(model)\n",
        "print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), hparams['learning_rate'])\n",
        "criterion = torch.nn.CTCLoss(blank=4000, zero_infinity=False).to(device)\n",
        "scheduler = optim.lr_scheduler.OneCycleLR(optimizer, max_lr=hparams['learning_rate'], \n",
        "                                        steps_per_epoch=int(len(train_loader)),\n",
        "                                        epochs=hparams['epochs'],\n",
        "                                        anneal_strategy='linear')\n",
        "\n",
        "# for epoch in range(1, epochs + 1):\n",
        "#     !date\n",
        "#     train(model, device, train_loader, criterion, optimizer, scheduler, epoch)\n",
        "#     test(model, device, test_loader, criterion, epoch)\n",
        "#     torch.save(model, \"/content/drive/MyDrive/MADE_ex/lab4/s2t_model.pth\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TransformerModel(\n",
            "  (conv_in): Sequential(\n",
            "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (3): ReLU()\n",
            "  )\n",
            "  (conv_out): Sequential(\n",
            "    (0): Linear(in_features=640, out_features=360, bias=True)\n",
            "    (1): PositionalEncoding(\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (encoder_layer): MultiSequential(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attn): MultiHeadedAttention(\n",
            "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (feed_forward): PositionwiseFeedForward(\n",
            "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
            "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (concat_linear): Sequential()\n",
            "    )\n",
            "    (1): EncoderLayer(\n",
            "      (self_attn): MultiHeadedAttention(\n",
            "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (feed_forward): PositionwiseFeedForward(\n",
            "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
            "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (concat_linear): Sequential()\n",
            "    )\n",
            "    (2): EncoderLayer(\n",
            "      (self_attn): MultiHeadedAttention(\n",
            "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (feed_forward): PositionwiseFeedForward(\n",
            "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
            "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (concat_linear): Sequential()\n",
            "    )\n",
            "    (3): EncoderLayer(\n",
            "      (self_attn): MultiHeadedAttention(\n",
            "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (feed_forward): PositionwiseFeedForward(\n",
            "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
            "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (concat_linear): Sequential()\n",
            "    )\n",
            "    (4): EncoderLayer(\n",
            "      (self_attn): MultiHeadedAttention(\n",
            "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (feed_forward): PositionwiseFeedForward(\n",
            "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
            "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (concat_linear): Sequential()\n",
            "    )\n",
            "    (5): EncoderLayer(\n",
            "      (self_attn): MultiHeadedAttention(\n",
            "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (feed_forward): PositionwiseFeedForward(\n",
            "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
            "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (concat_linear): Sequential()\n",
            "    )\n",
            "    (6): EncoderLayer(\n",
            "      (self_attn): MultiHeadedAttention(\n",
            "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (feed_forward): PositionwiseFeedForward(\n",
            "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
            "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (concat_linear): Sequential()\n",
            "    )\n",
            "    (7): EncoderLayer(\n",
            "      (self_attn): MultiHeadedAttention(\n",
            "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (feed_forward): PositionwiseFeedForward(\n",
            "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
            "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (concat_linear): Sequential()\n",
            "    )\n",
            "    (8): EncoderLayer(\n",
            "      (self_attn): MultiHeadedAttention(\n",
            "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (feed_forward): PositionwiseFeedForward(\n",
            "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
            "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (concat_linear): Sequential()\n",
            "    )\n",
            "    (9): EncoderLayer(\n",
            "      (self_attn): MultiHeadedAttention(\n",
            "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (feed_forward): PositionwiseFeedForward(\n",
            "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
            "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (norm1): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (concat_linear): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (after_norm): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "  (final_layer): Linear(in_features=360, out_features=4001, bias=True)\n",
            ")\n",
            "Num Model Parameters 14284849\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "YC84vwLfL4wm",
        "outputId": "fa89aa0d-f361-4b9b-cf00-46d8d09fcf33"
      },
      "source": [
        "# for epoch in range(1, epochs + 1):\n",
        "#     !date\n",
        "#     train(model, device, train_loader, criterion, optimizer, scheduler, epoch)\n",
        "#     test(model, device, test_loader, criterion, epoch)\n",
        "#     torch.save(model, \"/content/drive/MyDrive/MADE_ex/lab4/s2t_model.pth\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri May  7 18:55:52 UTC 2021\n",
            "Train Epoch: 1 [0/28539 (0%)]\tLoss: 53.807098\tLR: 0.000040\n",
            "Train Epoch: 1 [1600/28539 (6%)]\tLoss: 7.361096\tLR: 0.000063\n",
            "Train Epoch: 1 [3200/28539 (11%)]\tLoss: 6.877411\tLR: 0.000085\n",
            "Train Epoch: 1 [4800/28539 (17%)]\tLoss: 6.808859\tLR: 0.000108\n",
            "Train Epoch: 1 [6400/28539 (22%)]\tLoss: 6.962677\tLR: 0.000130\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-5d6203702b1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/MADE_ex/lab4/s2t_model.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-ac68320e0d96>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, criterion, optimizer, scheduler, epoch)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdata_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mspectrograms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mspectrograms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspectrograms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#(batch, 1, feat_dim, time)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1182\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1183\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1184\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1136\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1138\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1139\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tTyra8uD5Uy"
      },
      "source": [
        "epoch = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        },
        "id": "IFwkboWeAJef",
        "outputId": "cddcc4ad-47c3-4fdf-f8c1-f6156484ef54"
      },
      "source": [
        "model.train()\n",
        "data_len = len(train_loader.dataset)\n",
        "\n",
        "for batch_idx, _data in enumerate(train_loader):\n",
        "    spectrograms, labels, input_lengths, label_lengths = _data \n",
        "    spectrograms, labels = spectrograms[:, :, :,:max(input_lengths)].to(device), labels.to(device) #(batch, 1, feat_dim, time)\n",
        "    spectrograms = spectrograms.squeeze(1).transpose(1,2) # (batch, time, feat_dim,)\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    output = model(spectrograms, input_lengths)  # (batch, time, n_classes)\n",
        "    output = F.log_softmax(output, dim=2)\n",
        "    output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "    input_lengths = [x // 4 for x in input_lengths]\n",
        "\n",
        "    loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "    loss.backward()\n",
        "    \n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    if batch_idx % 100 == 0 or batch_idx == data_len:\n",
        "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tLR: {:.6f}'.format(\n",
        "            epoch, batch_idx * len(spectrograms), data_len,\n",
        "            100. * batch_idx / len(train_loader), loss.item(), scheduler.get_last_lr()[0]))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/28539 (0%)]\tLoss: 53.217194\tLR: 0.000040\n",
            "Train Epoch: 1 [1200/28539 (4%)]\tLoss: 7.331156\tLR: 0.000057\n",
            "Train Epoch: 1 [2400/28539 (8%)]\tLoss: 7.020089\tLR: 0.000074\n",
            "Train Epoch: 1 [3600/28539 (13%)]\tLoss: 6.539978\tLR: 0.000091\n",
            "Train Epoch: 1 [4800/28539 (17%)]\tLoss: 6.709806\tLR: 0.000107\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-4c769d56b0a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm_\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mclip_coef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_norm\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_norm\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mclip_coef\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_coef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpsWQy9hHKwp",
        "outputId": "62e37ac3-6491-48a9-be66-70449e60eda0"
      },
      "source": [
        "label_lengths"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[27, 22, 63, 55, 43, 52, 58, 34, 43, 65, 40, 63, 44, 53, 50, 38]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xKlT2pPHHAO",
        "outputId": "c20b98fd-09e5-4f73-ee99-b14dbad09734"
      },
      "source": [
        "input_lengths"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[259,\n",
              " 124,\n",
              " 378,\n",
              " 365,\n",
              " 350,\n",
              " 356,\n",
              " 369,\n",
              " 249,\n",
              " 305,\n",
              " 413,\n",
              " 257,\n",
              " 398,\n",
              " 364,\n",
              " 401,\n",
              " 396,\n",
              " 319]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUjRn261GEu_",
        "outputId": "c8430670-fe14-4a20-b041-094eda6e8f16"
      },
      "source": [
        "output[0][11]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ -8.3092, -13.9528, -13.1260,  ..., -10.3674, -12.4683,  -0.1574],\n",
              "       device='cuda:0', grad_fn=<SelectBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2h5mwPFSFPpz",
        "outputId": "02d9aa23-ae13-46c8-f962-78558eb3cb7c"
      },
      "source": [
        "output.shape"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([414, 12, 4001])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g37CSkOlFjtw",
        "outputId": "e76dbd1c-373d-49de-b3fa-720f3fea45ff"
      },
      "source": [
        "labels.shape"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12, 62])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BN0GYvmEaGJ"
      },
      "source": [
        "tmp = labels.cpu().numpy()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "DJuxWLQsE4K2",
        "outputId": "a5e78b84-977e-462c-d17f-72fe07932cd1"
      },
      "source": [
        "text_transform.int_to_text([0])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' ⁇ '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdl1v24mEmlp",
        "outputId": "8b24374e-d764-4f65-a6e8-d5b2c97a7ab7"
      },
      "source": [
        "tmp[1]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([3126.,  398.,   14., 2729.,   93.,   98.,    6., 2715., 1365.,\n",
              "         59., 1199.,   18.,  390., 1292.,   57.,   25., 1501.,   64.,\n",
              "        124.,  381., 1645.,   37., 1583.,  688.,  147.,    6., 2232.,\n",
              "        167.,  228.,   47.,  911., 1351., 3981.,   26.,  225.,  213.,\n",
              "       3981.,   64.,   57.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
              "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.,\n",
              "          0.,    0.,    0.,    0.,    0.,    0.,    0.,    0.],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBMP-UqdKJqI"
      },
      "source": [
        "decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zof6RjFDKLIJ",
        "outputId": "fbc6f640-bd88-4b94-dc60-5f571477e28a"
      },
      "source": [
        "decoded_preds"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '', '', '', '', '', '', '', '', '', '', '']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDfhhMavasQO",
        "outputId": "13da6871-46b7-4963-b5de-244b35a7b020"
      },
      "source": [
        "decoded_targets"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the next a gush of gladness would swell her heart at the thought that now she had him at least safer for a while and that he might die and so escape the whole crowd of horrible possibilities',\n",
              " 'remarked mister baxter at the tea table that evening i came past it today on my way cross lots home from the woods there will be bushels of plums on it',\n",
              " 'you must have a vast and magnificent estate said candide to the turk i have only twenty acres replied the old man i and my children cultivate them our labour preserves us from three great evils weariness vice and want',\n",
              " 'i saw a meadow lark on the first of march this day i heard blue birds and robins singing gaily it looked as though spring had come to stay i expected that day to reach dalton only eight miles distant',\n",
              " 'and its situation is beyond all words of mine to describe i greatly admired the pulpit which is supported by five pillars sunk into the backs of squashed lions but mister copley when i asked him the period said pure brummagem',\n",
              " 'the empyrean with the swans and the eagles even if you do have to fall back on the morrow into the bourgeoisie of the frogs',\n",
              " 'and we knew well that nothing could happen without our noticing it at three in the morning the sun cut through the clouds and we through the tent door to take in the situation was more than the work of a moment',\n",
              " 'and there he would find a man sitting',\n",
              " 'secondly the captain a surly and silent man had brought hither perhaps by force a young woman as his wife who was so unhappy that she pined away and died who was this woman',\n",
              " 'after the dance and other grand rejoicings both of the singing and the flaming forth effulgence with effulgence blithe and tender together at once with one accord had stopped even as the eyes that as volition moves them must needs together shut and lift themselves',\n",
              " 'why and how were the battles of shevardino and borodino given and accepted why was the battle of borodino fought there was not the least sense in it for either the french or the russians its immediate result for the russians was and was bound to be',\n",
              " 'that an involuntary terror sweeps over me and my lungs still seem short of air meanwhile']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mby39YVqZadd"
      },
      "source": [
        "### <b>Задание №1</b> (5 баллов):\n",
        "На данный момент практически все E2E SOTA решения использую сабворды (subwords/wordpieces) в качестве таргетов нейронки для распознавания. Нам бы тоже не мешало перейти от графем к сабвордам. Теперь вместо букв (графем) будем распознавать кусочки слов. В качестве такого токенайзера предлагается использовать [Sentencepiece](https://github.com/google/sentencepiece). Главное правильно обернуть его в наш класс TextTransform. Текстовый файл (train_clean_100_text_clean.txt) для обучения токенайзера уже подготовлен и лежит в корневой папке проекта. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZ5goXp4pn07"
      },
      "source": [
        "lines = []\n",
        "with open(\"/content/lab4/train_clean_100_text_clean.txt\", \"rt\") as ft:\n",
        "  for line in ft.readlines():\n",
        "      lines.append(line.lower())\n",
        "with open(\"/content/lab4/train_clean_100_text_clean_lower.txt\", \"wt\") as ftt:\n",
        "    for line in lines:\n",
        "        ftt.write(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LSc46Cnqop2"
      },
      "source": [
        "lines1 = []\n",
        "with open(\"/content/lab4/train_clean_100_text_clean_lower.txt\", \"rt\") as ft:\n",
        "  for line in ft.readlines():\n",
        "      lines1.append(line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4QV1UlUqwWW",
        "outputId": "97720483-4cde-4f3d-878c-f1a42bee26f3"
      },
      "source": [
        "len(lines) == len(lines1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOfTR4L2rBVV"
      },
      "source": [
        "del lines, lines1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNWwq7tNochP",
        "outputId": "b906498e-49ce-4bf0-c35c-e6eff033aef2"
      },
      "source": [
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 4.3MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNbiW919e2le"
      },
      "source": [
        "import sentencepiece as spm\n",
        "\n",
        "class TextTransformBPE:\n",
        "    def __init__(self, train_text_path='/content/lab4/train_clean_100_text_clean_lower.txt'):\n",
        "        \"\"\" Обучение BPE модели на 4000 юнитов\"\"\"\n",
        "        spm.SentencePieceTrainer.train(input=train_text_path,\n",
        "                                       model_prefix='bpe', vocab_size=4000,\n",
        "                                       model_type=\"bpe\",\n",
        "                                       normalization_rule_name=\"nfkc_cf\",\n",
        "                                       )\n",
        "        self.model = spm.SentencePieceProcessor(model_file='/content/lab4/bpe.model')\n",
        "\n",
        "    def text_to_int(self, text):\n",
        "        \"\"\" Преобразование входного текста в последовательность сабвордов в формате их индекса в BPE модели \"\"\"\n",
        "        int_sequence = self.model.encode(text)\n",
        "        return int_sequence\n",
        "\n",
        "    def int_to_text(self, labels):\n",
        "        \"\"\" Преобразование последовательности индексов сабвордов в текст \"\"\"\n",
        "        string = self.model.decode(labels)\n",
        "        return string"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXbb58FOvpRU"
      },
      "source": [
        "train_audio_transforms = torch.nn.Sequential(\n",
        "    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_fft=400, hop_length=160, n_mels=80),\n",
        "    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
        "    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
        ")\n",
        "\n",
        "valid_audio_transforms = torchaudio.transforms.MelSpectrogram(sample_rate=16000,\n",
        "                                                              n_fft=400,\n",
        "                                                              hop_length=160,\n",
        "                                                              n_mels=80)\n",
        "\n",
        "text_transform = TextTransformBPE()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICcagTURSG-Y"
      },
      "source": [
        "text_transform = TextTransform()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osOjLnop_wSN"
      },
      "source": [
        "def GreedyDecoder(output, labels, label_lengths, blank_label=4000, collapse_repeated=True):\n",
        "    # можно бимсерч прикрутить\n",
        "    arg_maxes = torch.argmax(output, dim=2)\n",
        "    decodes = []\n",
        "    targets = []\n",
        "    for i, args in enumerate(arg_maxes):\n",
        "        decode = []\n",
        "        targets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].long().tolist()))\n",
        "        for j, index in enumerate(args):\n",
        "            if index != blank_label:\n",
        "                if collapse_repeated and j != 0 and index == args[j -1]:\n",
        "                    continue\n",
        "                decode.append(index.item())\n",
        "        decodes.append(text_transform.int_to_text(decode))\n",
        "    return decodes, targets"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV48Q7HqZsAD"
      },
      "source": [
        "### <b>Задание №2</b> (5 баллов):\n",
        "Импровизация по улучшению качества распознавания."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4REA-ZZHxN2X"
      },
      "source": [
        "from espnet.nets.pytorch_backend.conformer.convolution import ConvolutionModule\n",
        "from espnet.nets.pytorch_backend.conformer.encoder_layer import EncoderLayer\n",
        "\n",
        "class ConformerModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=80,\n",
        "        output_size=29,\n",
        "        conv2d_filters=32,\n",
        "        attention_dim=360,\n",
        "        attention_heads=8,\n",
        "        cnn_module_kernel=31,\n",
        "        feedforward_dim=1024,\n",
        "        num_layers=10,\n",
        "        dropout=0.1,\n",
        "    ):\n",
        "        super(ConformerModel, self).__init__()\n",
        "        \n",
        "        self.conv_in = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(1, conv2d_filters, kernel_size=(3,3), stride=(2,2), padding=(1,1)),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Conv2d(conv2d_filters, conv2d_filters, kernel_size=(3,3), stride=(2,2), padding=(1,1)),\n",
        "            torch.nn.ReLU(),\n",
        "        )\n",
        "        self.conv_out = torch.nn.Sequential(\n",
        "            torch.nn.Linear(conv2d_filters * ((input_size // 2) // 2), attention_dim),\n",
        "            PositionalEncoding(attention_dim, 0.1),\n",
        "        )\n",
        "        positionwise_layer = PositionwiseFeedForward\n",
        "        positionwise_layer_args = (attention_dim, feedforward_dim, dropout)\n",
        "        convolution_layer = ConvolutionModule\n",
        "        convolution_layer_args = (attention_dim, cnn_module_kernel)\n",
        "\n",
        "        self.encoder_layer = repeat(\n",
        "            num_layers,\n",
        "            lambda lnum: EncoderLayer(\n",
        "                attention_dim,\n",
        "                MultiHeadedAttention(\n",
        "                    attention_heads, attention_dim, dropout\n",
        "                ),\n",
        "                positionwise_layer(*positionwise_layer_args),\n",
        "                None,\n",
        "                convolution_layer(*convolution_layer_args),\n",
        "                dropout,\n",
        "                normalize_before=True,\n",
        "                concat_after=False,\n",
        "            ),\n",
        "        )\n",
        "        self.after_norm = LayerNorm(attention_dim)\n",
        "        self.final_layer = torch.nn.Linear(attention_dim, output_size)\n",
        "\n",
        "    def forward(self, x, ilens):\n",
        "        x = x.unsqueeze(1)  # (b, c, t, f)\n",
        "        x = self.conv_in(x)\n",
        "        b, c, t, f = x.size()\n",
        "        x = self.conv_out(x.transpose(1, 2).contiguous().view(b, t, c * f))\n",
        "        masks = (~make_pad_mask(ilens)[:, None, :])[:, :, ::4].to(x.device)\n",
        "        x, _, _ = self.encoder_layer(x, masks)\n",
        "        x = self.after_norm(x)\n",
        "        x = self.final_layer(x)\n",
        "        return x"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0xpZgjhCwTxU",
        "outputId": "99139953-69c6-4fa4-fd06-912c6df9c1f5"
      },
      "source": [
        "text_transform = TextTransform()\n",
        "\n",
        "learning_rate = 1e-3\n",
        "batch_size = 12\n",
        "test_batch_size = 12\n",
        "epochs = 8\n",
        "libri_train_set = \"train-clean-100\"\n",
        "libri_test_set = \"test-clean\"\n",
        "torch.cuda.empty_cache()\n",
        "main(learning_rate, batch_size, test_batch_size, epochs, libri_train_set, libri_test_set,\n",
        "     vocab_size=29,\n",
        "     transformer=\"conformer\"\n",
        "     )"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ConformerModel(\n",
            "  (conv_in): Sequential(\n",
            "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (1): ReLU()\n",
            "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (3): ReLU()\n",
            "  )\n",
            "  (conv_out): Sequential(\n",
            "    (0): Linear(in_features=640, out_features=360, bias=True)\n",
            "    (1): PositionalEncoding(\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (encoder_layer): MultiSequential(\n",
            "    (0): EncoderLayer(\n",
            "      (self_attn): MultiHeadedAttention(\n",
            "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (feed_forward): PositionwiseFeedForward(\n",
            "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
            "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (conv_module): ConvolutionModule(\n",
            "        (pointwise_conv1): Conv1d(360, 720, kernel_size=(1,), stride=(1,))\n",
            "        (depthwise_conv): Conv1d(360, 360, kernel_size=(31,), stride=(1,), padding=(15,), groups=360)\n",
            "        (norm): BatchNorm1d(360, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (pointwise_conv2): Conv1d(360, 360, kernel_size=(1,), stride=(1,))\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (norm_ff): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm_mha): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm_conv): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm_final): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (concat_linear): Sequential()\n",
            "    )\n",
            "    (1): EncoderLayer(\n",
            "      (self_attn): MultiHeadedAttention(\n",
            "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (feed_forward): PositionwiseFeedForward(\n",
            "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
            "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (conv_module): ConvolutionModule(\n",
            "        (pointwise_conv1): Conv1d(360, 720, kernel_size=(1,), stride=(1,))\n",
            "        (depthwise_conv): Conv1d(360, 360, kernel_size=(31,), stride=(1,), padding=(15,), groups=360)\n",
            "        (norm): BatchNorm1d(360, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (pointwise_conv2): Conv1d(360, 360, kernel_size=(1,), stride=(1,))\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (norm_ff): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm_mha): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm_conv): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm_final): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (concat_linear): Sequential()\n",
            "    )\n",
            "    (2): EncoderLayer(\n",
            "      (self_attn): MultiHeadedAttention(\n",
            "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (feed_forward): PositionwiseFeedForward(\n",
            "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
            "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (conv_module): ConvolutionModule(\n",
            "        (pointwise_conv1): Conv1d(360, 720, kernel_size=(1,), stride=(1,))\n",
            "        (depthwise_conv): Conv1d(360, 360, kernel_size=(31,), stride=(1,), padding=(15,), groups=360)\n",
            "        (norm): BatchNorm1d(360, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (pointwise_conv2): Conv1d(360, 360, kernel_size=(1,), stride=(1,))\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (norm_ff): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm_mha): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm_conv): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm_final): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (concat_linear): Sequential()\n",
            "    )\n",
            "    (3): EncoderLayer(\n",
            "      (self_attn): MultiHeadedAttention(\n",
            "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (feed_forward): PositionwiseFeedForward(\n",
            "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
            "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (conv_module): ConvolutionModule(\n",
            "        (pointwise_conv1): Conv1d(360, 720, kernel_size=(1,), stride=(1,))\n",
            "        (depthwise_conv): Conv1d(360, 360, kernel_size=(31,), stride=(1,), padding=(15,), groups=360)\n",
            "        (norm): BatchNorm1d(360, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (pointwise_conv2): Conv1d(360, 360, kernel_size=(1,), stride=(1,))\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (norm_ff): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm_mha): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm_conv): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm_final): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (concat_linear): Sequential()\n",
            "    )\n",
            "    (4): EncoderLayer(\n",
            "      (self_attn): MultiHeadedAttention(\n",
            "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (feed_forward): PositionwiseFeedForward(\n",
            "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
            "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (conv_module): ConvolutionModule(\n",
            "        (pointwise_conv1): Conv1d(360, 720, kernel_size=(1,), stride=(1,))\n",
            "        (depthwise_conv): Conv1d(360, 360, kernel_size=(31,), stride=(1,), padding=(15,), groups=360)\n",
            "        (norm): BatchNorm1d(360, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (pointwise_conv2): Conv1d(360, 360, kernel_size=(1,), stride=(1,))\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (norm_ff): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm_mha): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm_conv): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm_final): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (concat_linear): Sequential()\n",
            "    )\n",
            "    (5): EncoderLayer(\n",
            "      (self_attn): MultiHeadedAttention(\n",
            "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (feed_forward): PositionwiseFeedForward(\n",
            "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
            "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (conv_module): ConvolutionModule(\n",
            "        (pointwise_conv1): Conv1d(360, 720, kernel_size=(1,), stride=(1,))\n",
            "        (depthwise_conv): Conv1d(360, 360, kernel_size=(31,), stride=(1,), padding=(15,), groups=360)\n",
            "        (norm): BatchNorm1d(360, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (pointwise_conv2): Conv1d(360, 360, kernel_size=(1,), stride=(1,))\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (norm_ff): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm_mha): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm_conv): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm_final): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (concat_linear): Sequential()\n",
            "    )\n",
            "    (6): EncoderLayer(\n",
            "      (self_attn): MultiHeadedAttention(\n",
            "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (feed_forward): PositionwiseFeedForward(\n",
            "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
            "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (conv_module): ConvolutionModule(\n",
            "        (pointwise_conv1): Conv1d(360, 720, kernel_size=(1,), stride=(1,))\n",
            "        (depthwise_conv): Conv1d(360, 360, kernel_size=(31,), stride=(1,), padding=(15,), groups=360)\n",
            "        (norm): BatchNorm1d(360, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (pointwise_conv2): Conv1d(360, 360, kernel_size=(1,), stride=(1,))\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (norm_ff): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm_mha): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm_conv): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm_final): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (concat_linear): Sequential()\n",
            "    )\n",
            "    (7): EncoderLayer(\n",
            "      (self_attn): MultiHeadedAttention(\n",
            "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (feed_forward): PositionwiseFeedForward(\n",
            "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
            "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (conv_module): ConvolutionModule(\n",
            "        (pointwise_conv1): Conv1d(360, 720, kernel_size=(1,), stride=(1,))\n",
            "        (depthwise_conv): Conv1d(360, 360, kernel_size=(31,), stride=(1,), padding=(15,), groups=360)\n",
            "        (norm): BatchNorm1d(360, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (pointwise_conv2): Conv1d(360, 360, kernel_size=(1,), stride=(1,))\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (norm_ff): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm_mha): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm_conv): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm_final): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (concat_linear): Sequential()\n",
            "    )\n",
            "    (8): EncoderLayer(\n",
            "      (self_attn): MultiHeadedAttention(\n",
            "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (feed_forward): PositionwiseFeedForward(\n",
            "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
            "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (conv_module): ConvolutionModule(\n",
            "        (pointwise_conv1): Conv1d(360, 720, kernel_size=(1,), stride=(1,))\n",
            "        (depthwise_conv): Conv1d(360, 360, kernel_size=(31,), stride=(1,), padding=(15,), groups=360)\n",
            "        (norm): BatchNorm1d(360, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (pointwise_conv2): Conv1d(360, 360, kernel_size=(1,), stride=(1,))\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (norm_ff): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm_mha): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm_conv): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm_final): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (concat_linear): Sequential()\n",
            "    )\n",
            "    (9): EncoderLayer(\n",
            "      (self_attn): MultiHeadedAttention(\n",
            "        (linear_q): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_k): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_v): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (linear_out): Linear(in_features=360, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (feed_forward): PositionwiseFeedForward(\n",
            "        (w_1): Linear(in_features=360, out_features=1024, bias=True)\n",
            "        (w_2): Linear(in_features=1024, out_features=360, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (conv_module): ConvolutionModule(\n",
            "        (pointwise_conv1): Conv1d(360, 720, kernel_size=(1,), stride=(1,))\n",
            "        (depthwise_conv): Conv1d(360, 360, kernel_size=(31,), stride=(1,), padding=(15,), groups=360)\n",
            "        (norm): BatchNorm1d(360, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (pointwise_conv2): Conv1d(360, 360, kernel_size=(1,), stride=(1,))\n",
            "        (activation): ReLU()\n",
            "      )\n",
            "      (norm_ff): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm_mha): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm_conv): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (norm_final): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "      (concat_linear): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (after_norm): LayerNorm((360,), eps=1e-12, elementwise_affine=True)\n",
            "  (final_layer): Linear(in_features=360, out_features=29, bias=True)\n",
            ")\n",
            "Num Model Parameters 16886557\n",
            "Fri May  7 20:14:01 UTC 2021\n",
            "Train Epoch: 1 [0/28539 (0%)]\tLoss: 4.150394\tLR: 0.000040\n",
            "Train Epoch: 1 [1200/28539 (4%)]\tLoss: 2.864961\tLR: 0.000057\n",
            "Train Epoch: 1 [2400/28539 (8%)]\tLoss: 2.880903\tLR: 0.000074\n",
            "Train Epoch: 1 [3600/28539 (13%)]\tLoss: 2.845811\tLR: 0.000091\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-fc51a850a2f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m main(learning_rate, batch_size, test_batch_size, epochs, libri_train_set, libri_test_set,\n\u001b[1;32m     11\u001b[0m      \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m29\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m      \u001b[0mtransformer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"conformer\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m      )\n",
            "\u001b[0;32m<ipython-input-7-9a8530434ee4>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(learning_rate, batch_size, test_batch_size, epochs, train_url, test_url, vocab_size, transformer)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/MADE_ex/lab4/s2t_model.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-ac68320e0d96>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, criterion, optimizer, scheduler, epoch)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0minput_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, log_probs, targets, input_lengths, target_lengths)\u001b[0m\n\u001b[1;32m   1589\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_probs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_lengths\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m         return F.ctc_loss(log_probs, targets, input_lengths, target_lengths, self.blank, self.reduction,\n\u001b[0;32m-> 1591\u001b[0;31m                           self.zero_infinity)\n\u001b[0m\u001b[1;32m   1592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m \u001b[0;31m# TODO: L1HingeEmbeddingCriterion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mctc_loss\u001b[0;34m(log_probs, targets, input_lengths, target_lengths, blank, reduction, zero_infinity)\u001b[0m\n\u001b[1;32m   2306\u001b[0m     \"\"\"\n\u001b[1;32m   2307\u001b[0m     return torch.ctc_loss(\n\u001b[0;32m-> 2308\u001b[0;31m         \u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_infinity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2309\u001b[0m     )\n\u001b[1;32m   2310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c86JKPPzdyoD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}